---
layout: default
---

# Introduction to Deep Learning

Deep Learning is a rapidly expanding field with new applications found every day. In this workshop, we will cover the fundamentals of deep learning for the beginner. 

We will introduce the math behind training deep learning models: the back-propagation algorithm. Building conceptual understanding of the fundamentals of deep learning will be the focus of the first part of the workshop. We will then cover some of the popular architectures used in deep learning, such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), LSTMs, autoencoders and GANs. 

There will be a hands-on computing tutorial using Jupyter notebooks to build a basic image classification model via transfer learning.  By the end of the workshop, participants will have a firm understanding of the basic terminology and jargon of deep learning and will be prepared to dive into the plethora of online resources and literature available for each specific application area.


## About the Instructor

![Aashwin Mishra](/assets/img/aashwin.png){:style="max-width:30%;"}

Aashwin Mishra is a Project Scientist at the Machine Learning Initiative at the National Accelerator Laboratory (SLAC). His research focuses on uncertainty quantification, probabilistic modeling, interpretability/explainability, and optimization across physics applications.

# Workshop Materials

## Pre-workshop Checklist

The workshop assumes that you have requisite knowledge of: 

a) Python programming 

b) Numpy 

c) basic concepts of Machine Learning (desirable) 


For (a), you should have written codes in Python, and at some juncture, defined and used a class. 

For (b), you should be comfortable in basic linear algebra operations in Numpy, understand broadcasting, etc. 

(c) is desirable, but not required. 

## Schedule

#### Session 1 (Tuesday, August 9 1:00 PM - 4:00 PM PDT)

-[Recording of session 1](https://stanford.zoom.us/rec/share/RnyMtZprvqt5koJUADNoleea81GxBE_W5AR0-Cla3EXwfkFJes1jiGH4icVowfAI.BL2ezLklu15PzXoK?startTime=1660074060000)

-Review of basic ML concepts

-Overview of deep learning

-Coding a perceptron in numpy

-Understanding fully connected neural networks

-Coding fully connected neural networks in numpy

-Introduction to Torch

-Using the AD facilities in torch to train models

-Using the sub-modules in torch to define and train models
  
#### Session 2 (Wednesday, August 10 1:00 PM - 4:00 PM PDT)

-[Recording of session 2](https://stanford.zoom.us/rec/share/rWi9bweXG3QJvF7M1rURkEHcSn_kN2Cw7pK1SJjxpFJqnTg7hPnDG7-K41dG6kM0.Vq_VzePu5vP1HGJA?startTime=1660160664000)

-Understanding Convolutional Neural Networks

-Defining and Training CNNs in torch 

-Using GPUs for training

-Data Augmentation 

#### Google Colab notebooks

-[Linear Regression Exercise in Numpy:](https://colab.research.google.com/drive/1w0C62ikCOotfBJ5FbzQu4I3weu6viAmj?usp=sharing)

-[Solutions: Linear Regression Exercise in Numpy](https://colab.research.google.com/drive/1w0C62ikCOotfBJ5FbzQu4I3weu6viAmj?usp=sharing)

-[Shallow Neural Network in Numpy:](https://colab.research.google.com/drive/1mbquyEd_N_JMh8nTupbXgId1ArVZuP3L?usp=sharing)

-[Solutions: Shallow Neural Network in Numpy](https://colab.research.google.com/drive/1mbquyEd_N_JMh8nTupbXgId1ArVZuP3L?usp=sharing)

-[Introduction to Torch](https://colab.research.google.com/drive/1b1ifUhsdo7rYeUEKBjEQkWTgWX0EgEz6?usp=sharing)

-[My First Torch Model:](https://colab.research.google.com/drive/1GLihAAAmsz1Snqt2GLg55hSO0UQWBGLM?usp=sharing)

-[MNIST with an FCNN:](https://colab.research.google.com/drive/1Wp2jWYnZ50VWBPCVEkPemUcF3ohrxrct?usp=sharing)

-[CIFAR10 with CNNs:](https://colab.research.google.com/drive/1eZniJ3FW77cAy4U3cSieJPSq-ukMARPY?usp=sharing)

-[CIFAR10 with GPUs:](https://colab.research.google.com/drive/153nTZtmHENNTx-XLWw3kl41Shd-ZvXVJ?usp=sharing)

-[Data Augmentation for Computer Vision:](https://colab.research.google.com/drive/1Ug0STBPfwc0Q7YSBasliIJCC38y9pOVm?usp=sharing)
