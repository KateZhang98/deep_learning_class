---
layout: default
---

# Introduction to Deep Learning

Deep Learning is a rapidly expanding field with new applications found every day. In this workshop, we will cover the fundamentals of deep learning for the beginner. 

We will introduce the math behind training deep learning models: the back-propagation algorithm. Building conceptual understanding of the fundamentals of deep learning will be the focus of the first part of the workshop. We will then cover some of the popular architectures used in deep learning, such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), LSTMs, autoencoders and GANs. 

There will be a hands-on computing tutorial using Jupyter notebooks to build a basic image classification model via transfer learning.  By the end of the workshop, participants will have a firm understanding of the basic terminology and jargon of deep learning and will be prepared to dive into the plethora of online resources and literature available for each specific application area.


## About the Instructor

![Aashwin Mishra](/assets/img/aashwin.png){:style="max-width:30%;"}

Aashwin Mishra is a Project Scientist at the Machine Learning Initiative at the National Accelerator Laboratory (SLAC). His research focuses on uncertainty quantification, probabilistic modeling, interpretability/explainability, and optimization across physics applications.

# Workshop Materials

## Pre-workshop Checklist

In progress

## Schedule

#### Session 1 (Tuesday, August 9 1:00 PM - 4:00 PM PDT)

In progress
  
#### Session 2 (Wednesday, August 10 1:00 PM - 4:00 PM PDT)

In progress
